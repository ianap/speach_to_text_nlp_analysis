{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Topic Modeling Pipeline with AWS Comprehend\n",
    "\n",
    "This Jupyter notebook implements a pipeline for topic modeling using AWS Comprehend. The pipeline takes as input a file containing interview questions and responses, and generates topics and corresponding keywords using the Comprehend topic modeling API.\n",
    "\n",
    "The pipeline consists of the following steps:\n",
    "\n",
    "1. __Data Preprocessing__: The input file is read and preprocessed to remove any unwanted characters or formatting. This is done using Python's pandas and re libraries.\n",
    "\n",
    "2. __Calling AWS Comprehend Topic Modeling API__: The preprocessed data is then passed to the Comprehend topic modeling API using the AWS SDK for Python (boto3). The API generates topics and corresponding keywords based on the input data.\n",
    "\n",
    "3. __Extracting Results__: The generated results are then extracted and stored in a pandas dataframe for further sentiment analysis.\n",
    "\n",
    "This pipeline can be customized to fit different types of input data and analysis requirements.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sagemaker ipywidgets --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend = boto3.client('comprehend', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read input file with interview questions and responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET='cnatest' # Or whatever you called your bucket\n",
    "data_key = 'transcript_with_mapped_questions_and_answers.csv' # Where the file is within your bucket\n",
    "data_location = 's3://{}/{}'.format(BUCKET, data_key)\n",
    "df = pd.read_csv(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce_to_question</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As a national savings for the healthcare syste...</td>\n",
       "      <td>if this is running correctly? And we get buy i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fees will be paid to GPS based on the health r...</td>\n",
       "      <td>start first. What I used to hear a lot of is t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From your point of view What is Healthier SG</td>\n",
       "      <td>I think healthier S. G. Is a whole rethinking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If there's one thing that you could you know c...</td>\n",
       "      <td>for me I think information to unify all the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Should the typical patient be concerned about ...</td>\n",
       "      <td>over time. We will see a general increase in c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                responce_to_question  \\\n",
       "0  As a national savings for the healthcare syste...   \n",
       "1  Fees will be paid to GPS based on the health r...   \n",
       "2       From your point of view What is Healthier SG   \n",
       "3  If there's one thing that you could you know c...   \n",
       "4  Should the typical patient be concerned about ...   \n",
       "\n",
       "                                                text  \n",
       "0  if this is running correctly? And we get buy i...  \n",
       "1  start first. What I used to hear a lot of is t...  \n",
       "2  I think healthier S. G. Is a whole rethinking ...  \n",
       "3  for me I think information to unify all the in...  \n",
       "4  over time. We will see a general increase in c...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformed review, input for Comprehend\n",
    "LOCAL_TRANSFORMED_INTERVIEW = os.path.join('data', 'Transformed.txt')\n",
    "S3_OUT = 's3://' + BUCKET + '/out/' + 'Transformed.txt'\n",
    "\n",
    "# Final dataframe where topics and sentiments are going to be joined\n",
    "S3_TOPICS = 's3://' + BUCKET + '/out/' + 'FinalDataframe.csv'\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce_to_question</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As a national savings for the healthcare syste...</td>\n",
       "      <td>if this is running correctly and we get buy in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fees will be paid to GPS based on the health r...</td>\n",
       "      <td>start first what  used to hear a lot of is tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From your point of view What is Healthier SG</td>\n",
       "      <td>think healthier s g s a whole rethinking of h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If there's one thing that you could you know c...</td>\n",
       "      <td>for me  think information to unify all the inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Should the typical patient be concerned about ...</td>\n",
       "      <td>over time we will see a general increase in co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The idea of having family doctors is something...</td>\n",
       "      <td>dont think were only doing it now  think weve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Will this also add on to increasing burdens fo...</td>\n",
       "      <td>be honest and say that it is going to be certa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>when you look at our health care system what w...</td>\n",
       "      <td>would actually just extend a little bit from...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                responce_to_question  \\\n",
       "0  As a national savings for the healthcare syste...   \n",
       "1  Fees will be paid to GPS based on the health r...   \n",
       "2       From your point of view What is Healthier SG   \n",
       "3  If there's one thing that you could you know c...   \n",
       "4  Should the typical patient be concerned about ...   \n",
       "5  The idea of having family doctors is something...   \n",
       "6  Will this also add on to increasing burdens fo...   \n",
       "7  when you look at our health care system what w...   \n",
       "\n",
       "                                                text  \n",
       "0  if this is running correctly and we get buy in...  \n",
       "1  start first what  used to hear a lot of is tha...  \n",
       "2   think healthier s g s a whole rethinking of h...  \n",
       "3  for me  think information to unify all the inf...  \n",
       "4  over time we will see a general increase in co...  \n",
       "5   dont think were only doing it now  think weve...  \n",
       "6  be honest and say that it is going to be certa...  \n",
       "7    would actually just extend a little bit from...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(df):\n",
    "    \"\"\"Preprocessing review text.\n",
    "    The text becomes Comprehend compatible as a result.\n",
    "    This is the most important preprocessing step.\n",
    "    \"\"\"\n",
    "    # Encode and decode reviews\n",
    "    df['text'] = df['text'].str.encode(\"utf-8\", \"ignore\")\n",
    "    df['text'] = df['text'].str.decode('ascii')\n",
    "    \n",
    "    df['text'] = df['text'].str.replace('[PII]','', regex=True)\n",
    "    # Replacing characters with whitespace\n",
    "    df['text'] = df['text'].replace(r'\\r+|\\n+|\\t+|\\u2028',' ', regex=True)\n",
    "\n",
    "    # Replacing punctuations\n",
    "    df['text'] = df['text'].str.replace('[^\\w\\s]','', regex=True)\n",
    "\n",
    "    # Lowercasing reviews\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    return df\n",
    "df = clean_text(df)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data(df):\n",
    "    \"\"\"Encoding and getting reviews in byte size.\n",
    "    Review gets encoded to utf-8 format and getting the size of the reviews in bytes. \n",
    "    Comprehend requires each review input to be no more than 5000 Bytes\n",
    "    \"\"\"\n",
    "    df['textsize'] = df['text'].apply(lambda x:len(x.encode('utf-8')))\n",
    "    df = df[(df['textsize'] > 0) & (df['textsize'] < 5000)]\n",
    "    df = df.drop(columns=['textsize'])\n",
    "    return df\n",
    "df = prepare_input_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first save the input file locally\n",
    "with open(LOCAL_TRANSFORMED_INTERVIEW, \"w\") as outfile:\n",
    "    outfile.write(\"\\n\".join(df['text'].tolist()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run an Amazon Comprehend topic modeling job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client and session information\n",
    "session = boto3.Session()\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Account id. Required downstream.\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# Initializing Comprehend client\n",
    "comprehend = boto3.client(service_name='comprehend', \n",
    "                          region_name=session.region_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of topics set to 5 after having a human-in-the-loop\n",
    "# This needs to be fully aligned with topicMaps dictionary in the third script \n",
    "NUMBER_OF_TOPICS = 5\n",
    "\n",
    "# Input file format of one review per line\n",
    "input_doc_format = \"ONE_DOC_PER_LINE\"\n",
    "\n",
    "# Role arn (Hard coded, masked)\n",
    "data_access_role_arn = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for S3 bucket and input data file\n",
    "BUCKET='cnatest' \n",
    "input_s3_url = 's3://' + BUCKET + '/out/' + 'Transformed.txt'\n",
    "output_s3_url = 's3://' + BUCKET + '/out/' + 'output/'\n",
    "\n",
    "# Final dataframe where we will join Comprehend outputs later\n",
    "S3_FEEDBACK_TOPICS = 's3://' + BUCKET + '/out/' + 'FinalDataframe.csv'\n",
    "\n",
    "# Local copy of Comprehend output\n",
    "LOCAL_COMPREHEND_OUTPUT_DIR = os.path.join('comprehend_out', '')\n",
    "LOCAL_COMPREHEND_OUTPUT_FILE = os.path.join(LOCAL_COMPREHEND_OUTPUT_DIR, 'output.tar.gz')\n",
    "\n",
    "INPUT_CONFIG={\n",
    "    # The S3 URI where Comprehend input is placed.\n",
    "    'S3Uri':    input_s3_url,\n",
    "    # Document format\n",
    "    'InputFormat': input_doc_format,\n",
    "}\n",
    "OUTPUT_CONFIG={\n",
    "    # The S3 URI where Comprehend output is placed.\n",
    "    'S3Uri':    output_s3_url,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_topics_detection_job_result\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Start Comprehend topic modelling job.\n",
    "# Specifies the number of topics, input and output config and IAM role ARN \n",
    "# that grants Amazon Comprehend read access to data.\n",
    "start_topics_detection_job_result = comprehend.start_topics_detection_job(\n",
    "                                                    NumberOfTopics=NUMBER_OF_TOPICS,\n",
    "                                                    InputDataConfig=INPUT_CONFIG,\n",
    "                                                    OutputDataConfig=OUTPUT_CONFIG,\n",
    "DataAccessRoleArn=data_access_role_arn)\n",
    "\n",
    "print('start_topics_detection_job_result' )\n",
    "\n",
    "# Job ID is required downstream for extracting the Comprehend results\n",
    "job_id = start_topics_detection_job_result[\"JobId\"]\n",
    "#print('job_id: ', job_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBMITTED\n",
      "IN_PROGRESS\n"
     ]
    }
   ],
   "source": [
    "# Keeping track if Comprehend has finished its job\n",
    "description = comprehend.describe_topics_detection_job(JobId=job_id)\n",
    "\n",
    "topic_detection_job_status = description['TopicsDetectionJobProperties'][\"JobStatus\"]\n",
    "print(topic_detection_job_status)\n",
    "while topic_detection_job_status not in [\"COMPLETED\", \"FAILED\"]:\n",
    "    time.sleep(120)\n",
    "    topic_detection_job_status = comprehend.describe_topics_detection_job(JobId=job_id)['TopicsDetectionJobProperties'][\"JobStatus\"]\n",
    "    print(topic_detection_job_status)\n",
    "\n",
    "topic_detection_job_status = comprehend.describe_topics_detection_job(JobId=job_id)['TopicsDetectionJobProperties'][\"JobStatus\"]\n",
    "print(topic_detection_job_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend.describe_topics_detection_job(JobId=job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the job is successfully complete, it returns a compressed archive containing two files: topic-terms.csv and doc-topics.csv. The first output file, topic-terms.csv, is a list of topics in the collection. For each topic, the list includes, by default, the top terms by topic according to their weight. The second file, doc-topics.csv, lists the documents associated with a topic and the proportion of the document that is concerned with the topic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs of Amazon Comprehend are copied locally for our next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket prefix where model artifacts are stored\n",
    "prefix = f'{account_id}-TOPICS-{job_id}'\n",
    "\n",
    "# Model artifact zipped file\n",
    "artifact_file = 'output.tar.gz'\n",
    "\n",
    "# Location on S3 where model artifacts are stored\n",
    "target = f's3://{BUCKET}/out/output/{prefix}/output/{artifact_file}'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Comprehend output from S3 to local notebook instance\n",
    "! aws s3 cp {target}  ./comprehend-out/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract output file and read them with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the Comprehend output file. \n",
    "# Two files are now saved locally- \n",
    "#       (1) comprehend-out/doc-topics.csv and \n",
    "#       (2) comprehend-out/topic-terms.csv\n",
    "\n",
    "comprehend_tars = tarfile.open('comprehend-out/output.tar.gz')\n",
    "comprehend_tars.extractall('comprehend-out')\n",
    "comprehend_tars.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = pd.read_csv('comprehend-out/topic-terms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs_by_topics = pd.read_csv('comprehend-out/doc-topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>term</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>direction</td>\n",
       "      <td>0.071109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>cost</td>\n",
       "      <td>0.057836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>good</td>\n",
       "      <td>0.031432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic       term    weight\n",
       "0      0  direction  0.071109\n",
       "1      0       cost  0.057836\n",
       "2      0       good  0.031432"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topics.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docname</th>\n",
       "      <th>topic</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformed.txt:0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.340430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformed.txt:0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.222851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformed.txt:0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.220077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformed.txt:0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.142298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformed.txt:0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.074345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformed.txt:3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformed.txt:2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformed.txt:1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Transformed.txt:5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.405211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Transformed.txt:5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.208956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Transformed.txt:5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.155403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Transformed.txt:5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.116095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Transformed.txt:5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.114335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Transformed.txt:6</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Transformed.txt:4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              docname  topic  proportion\n",
       "0   Transformed.txt:0      4    0.340430\n",
       "1   Transformed.txt:0      0    0.222851\n",
       "2   Transformed.txt:0      1    0.220077\n",
       "3   Transformed.txt:0      3    0.142298\n",
       "4   Transformed.txt:0      2    0.074345\n",
       "5   Transformed.txt:3      2    1.000000\n",
       "6   Transformed.txt:2      1    1.000000\n",
       "7   Transformed.txt:1      3    1.000000\n",
       "8   Transformed.txt:5      2    0.405211\n",
       "9   Transformed.txt:5      4    0.208956\n",
       "10  Transformed.txt:5      3    0.155403\n",
       "11  Transformed.txt:5      1    0.116095\n",
       "12  Transformed.txt:5      0    0.114335\n",
       "13  Transformed.txt:6      4    1.000000\n",
       "14  Transformed.txt:4      0    1.000000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_docs_by_topics.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall we have 5 topics. Lets look closer to terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   topic        term    weight\n",
      "0      0   direction  0.071109\n",
      "1      0        cost  0.057836\n",
      "2      0        good  0.031432\n",
      "3      0  scientific  0.021970\n",
      "4      0        work  0.021009\n",
      "5      0    diabetes  0.021630\n",
      "6      0     primary  0.021193\n",
      "7      0    hospital  0.020882\n",
      "8      0         key  0.020837\n",
      "9      0  prevention  0.020256\n",
      "    topic        term    weight\n",
      "10      1       focus  0.046187\n",
      "11      1     illness  0.046187\n",
      "12      1      doctor  0.043520\n",
      "13      1     chronic  0.044497\n",
      "14      1     healthy  0.042997\n",
      "15      1        back  0.033492\n",
      "16      1     disease  0.032519\n",
      "17      1  healthcare  0.032567\n",
      "18      1      health  0.044762\n",
      "19      1      school  0.021253\n",
      "    topic     term    weight\n",
      "20      2   system  0.064898\n",
      "21      2  vaccine  0.046083\n",
      "22      2    covid  0.033421\n",
      "23      2    unify  0.033421\n",
      "24      2     weve  0.040784\n",
      "25      2     good  0.030318\n",
      "26      2     huge  0.021204\n",
      "27      2      ago  0.021204\n",
      "28      2  blanket  0.021204\n",
      "29      2   safety  0.021204\n",
      "    topic       term    weight\n",
      "30      3    patient  0.044353\n",
      "31      3       case  0.028600\n",
      "32      3    outcome  0.026382\n",
      "33      3      check  0.023347\n",
      "34      3       bite  0.020458\n",
      "35      3  difficult  0.017064\n",
      "36      3       kind  0.016314\n",
      "37      3       base  0.016201\n",
      "38      3        let  0.010747\n",
      "39      3     estate  0.010747\n",
      "    topic       term    weight\n",
      "40      4     health  0.063152\n",
      "41      4      shift  0.037332\n",
      "42      4      early  0.031545\n",
      "43      4      thing  0.033195\n",
      "44      4        80s  0.023916\n",
      "45      4  longevity  0.023916\n",
      "46      4          t  0.023562\n",
      "47      4     people  0.022541\n",
      "48      4       long  0.022541\n",
      "49      4       love  0.023086\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    df_tmp = df_topics[df_topics.topic==i]\n",
    "    print(df_tmp.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic names for 5 topics created by human-in-the-loop or SME feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicMaps = {\n",
    "    0: 'Cost',\n",
    "    1: 'Managment of Chronic Conditions',\n",
    "    2: 'System Operation',\n",
    "    3: 'Patient Experience',\n",
    "    4: 'Changes in Healthcare',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed to the notebook _\"04_topic_mapping_sentiment.ipynb\"_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
